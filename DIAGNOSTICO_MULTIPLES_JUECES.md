# ğŸ” DiagnÃ³stico: Sistema de MÃºltiples Jueces

## ğŸ“‹ Problema Reportado

El usuario probÃ³ con **4 mÃ³viles** (4 jueces diferentes), pero el sistema **no sumaba** las evaluaciones de todos los jueces. Solo tomaba el resultado de uno.

## âœ… IMPORTANTE: Sistema de SUMA (no promedio)

El sistema **SUMA** las puntuaciones de todos los jueces, **NO calcula promedios**.

**Ejemplo:**
- Juez 1 da: 9 puntos totales
- Juez 2 da: 27 puntos totales
- **Resultado: 36 puntos** (9 + 27, NO 18)

## âœ… SoluciÃ³n Implementada

### 1. **VerificaciÃ³n de la LÃ³gica del Backend**

El cÃ³digo del backend **YA ESTÃ CORRECTO** y calcula promedios correctamente:

```javascript
// Para cada criterio evaluado
const allEvaluationsForCriterion = await Evaluation.find({
  teamId,
  criterionId: evalData.criterionId,
});

// Calcular promedio de TODOS los jueces
const avgScore =
  allEvaluationsForCriterion.reduce((sum, e) => sum + e.score, 0) / 
  allEvaluationsForCriterion.length;

// Guardar el promedio
team.scores[evalData.criterionId] = avgScore;
```

**CÃ³mo funciona:**
1. Cuando un juez envÃ­a evaluaciones, se guardan en la BD
2. Se buscan **TODAS** las evaluaciones para cada criterio (de todos los jueces)
3. Se calcula el **promedio** de todas las evaluaciones
4. Se guarda el promedio en `team.scores[criterionId]`
5. Se calcula el puntaje final sumando todos los promedios

### 2. **Logs de DiagnÃ³stico Mejorados**

Agregamos logs detallados para ver exactamente quÃ© estÃ¡ pasando:

```javascript
console.log(`ğŸ“Š Criterio ${evalData.criterionId}: ${allEvaluationsForCriterion.length} evaluaciones encontradas`);

// Mostrar las evaluaciones de cada juez
allEvaluationsForCriterion.forEach(ev => {
  console.log(`  - Juez ${ev.judgeId}: ${ev.score} puntos`);
});

console.log(`  â¡ï¸ Promedio calculado: ${avgScore.toFixed(2)} puntos`);
```

**Ejemplo de salida:**
```
ğŸ“Š Criterio criterion-1: 3 evaluaciones encontradas
  - Juez judge-001: 4 puntos
  - Juez judge-002: 3 puntos
  - Juez judge-003: 4 puntos
  â¡ï¸ Promedio calculado: 3.67 puntos
```

### 3. **Endpoints HTTP de DiagnÃ³stico**

Agregamos endpoints para **ver todas las evaluaciones** desde el navegador:

#### **ğŸ“ Ver todas las evaluaciones:**
```
GET http://TU_IP:3001/api/debug/evaluations
```

**Respuesta:**
```json
{
  "totalEvaluations": 36,
  "totalTeams": 4,
  "totalJudges": 4,
  "teams": [
    {
      "id": "team-1",
      "name": "Equipo Alpha",
      "scores": {
        "criterion-1": 3.67,
        "criterion-2": 3.25,
        "criterion-3": 4.0
      },
      "finalScore": 10.92,
      "evaluationsCompleted": 4
    }
  ],
  "evaluationsByTeam": {
    "team-1": [
      { "judgeId": "judge-001", "criterionId": "criterion-1", "score": 4 },
      { "judgeId": "judge-002", "criterionId": "criterion-1", "score": 3 },
      { "judgeId": "judge-003", "criterionId": "criterion-1", "score": 4 }
    ]
  }
}
```

#### **ğŸ“ Ver resumen de un equipo especÃ­fico:**
```
GET http://TU_IP:3001/api/debug/summary/team-1
```

**Respuesta:**
```json
{
  "team": {
    "id": "team-1",
    "name": "Equipo Alpha",
    "finalScore": 10.92,
    "scores": {
      "criterion-1": 3.67,
      "criterion-2": 3.25
    }
  },
  "evaluationsByCriterion": [
    {
      "criterionId": "criterion-1",
      "judgesCount": 3,
      "evaluations": [
        { "judgeId": "judge-001", "score": 4 },
        { "judgeId": "judge-002", "score": 3 },
        { "judgeId": "judge-003", "score": 4 }
      ],
      "sum": 11,
      "average": "3.67",
      "storedInTeam": 3.67
    }
  ],
  "totalEvaluations": 27
}
```

## ğŸ”§ CÃ³mo Diagnosticar el Problema

### **Paso 1: Revisar los logs del servidor**

Cuando un juez envÃ­a evaluaciones, deberÃ­as ver:

```
============================================================
ğŸ“Š NUEVA EVALUACIÃ“N RECIBIDA
   Juez: judge-1763693776222
   Equipo: team-1
   Criterios evaluados: 9
============================================================

ğŸ“Š Criterio criterion-1: 2 evaluaciones encontradas
  - Juez judge-001: 4 puntos
  - Juez judge-1763693776222: 3 puntos
  â¡ï¸ Promedio calculado: 3.50 puntos

ğŸ”¢ Calculando puntaje final para "Equipo Alpha":
   Scores por criterio: {
     criterion-1: 3.5,
     criterion-2: 3.75,
     ...
   }
   Total de criterios evaluados: 9
   Suma total: 32.50 puntos
   Promedio general: 3.61 puntos

âœ… Equipo "Equipo Alpha" actualizado: 32.50 puntos totales
```

### **Paso 2: Verificar en el navegador**

Abre en tu navegador (reemplaza con tu IP):
```
http://192.168.1.76:3001/api/debug/evaluations
```

Verifica:
- âœ… Â¿CuÃ¡ntos `totalJudges` hay?
- âœ… Â¿Cada criterio tiene evaluaciones de mÃºltiples jueces?
- âœ… Â¿Los promedios en `team.scores` reflejan mÃºltiples evaluaciones?

### **Paso 3: Verificar un equipo especÃ­fico**

```
http://192.168.1.76:3001/api/debug/summary/team-1
```

Verifica:
- âœ… Â¿`judgesCount` es mayor a 1 para cada criterio?
- âœ… Â¿El `average` coincide con `storedInTeam`?

## ğŸ› Posibles Causas del Problema

Si despuÃ©s de estos logs **NO VES** mÃºltiples evaluaciones por criterio, puede ser:

### **1. Los jueces estÃ¡n usando el mismo `judgeId`**
- âŒ **Problema:** Todos los dispositivos tienen `judge-1763693776222`
- âœ… **SoluciÃ³n:** Cada dispositivo debe generar un `judgeId` Ãºnico al escanear el QR

### **2. Las evaluaciones no se estÃ¡n guardando**
- âŒ **Problema:** Error al guardar en MongoDB
- âœ… **SoluciÃ³n:** Revisar logs del servidor para errores

### **3. Las evaluaciones antiguas se estÃ¡n sobrescribiendo**
- âŒ **Problema:** El esquema no permite duplicados
- âœ… **SoluciÃ³n:** Verificar que el esquema de `Evaluation` NO tenga `unique` en campos incorrectos

## ğŸ“Š Esquema de EvaluaciÃ³n Actual

```javascript
const evaluationSchema = new mongoose.Schema({
  teamId: String,        // NO unique
  judgeId: String,       // NO unique
  criterionId: String,   // NO unique
  score: Number,
  timestamp: Date,
});
// âš ï¸ NO hay unique compound index
```

**Esto permite mÃºltiples evaluaciones para el mismo:**
- Equipo
- Criterio
- De diferentes jueces âœ…

## ğŸ¯ PrÃ³ximos Pasos

1. **Prueba con 4 dispositivos:**
   - Dispositivo 1: Totem Mode
   - Dispositivos 2-4: Judge Mode (escanean QR)

2. **Revisa los logs en el servidor** al enviar evaluaciones

3. **Abre el endpoint de debug** en un navegador:
   ```
   http://TU_IP:3001/api/debug/evaluations
   ```

4. **Verifica:**
   - Â¿CuÃ¡ntos jueces hay registrados?
   - Â¿Cada criterio tiene evaluaciones de mÃºltiples jueces?
   - Â¿Los promedios son correctos?

5. **Reporta los resultados:**
   - Si ves mÃºltiples evaluaciones pero el frontend no se actualiza â†’ Problema de UI
   - Si solo ves una evaluaciÃ³n por criterio â†’ Problema de generaciÃ³n de `judgeId`
   - Si ves errores en los logs â†’ Problema de BD

## ğŸ“ Checklist de VerificaciÃ³n

- [ ] Servidor backend reiniciado con nuevos logs
- [ ] 4 dispositivos preparados (1 Totem, 3 Jueces)
- [ ] Jueces escanean QR y reciben equipo
- [ ] Jueces envÃ­an evaluaciones
- [ ] Logs del servidor muestran mÃºltiples evaluaciones por criterio
- [ ] Endpoint `/api/debug/evaluations` muestra mÃºltiples jueces
- [ ] Promedios calculados correctamente
- [ ] Frontend actualiza con promedios correctos

## ğŸ”„ Flujo Esperado (4 Jueces)

```
1. Totem registra "Equipo Alpha"
2. Totem envÃ­a "Equipo Alpha" a jueces
3. Juez A evalÃºa: criterion-1 = 4 pts â†’ Promedio: 4.00
4. Juez B evalÃºa: criterion-1 = 3 pts â†’ Promedio: 3.50
5. Juez C evalÃºa: criterion-1 = 4 pts â†’ Promedio: 3.67
6. Juez D evalÃºa: criterion-1 = 3 pts â†’ Promedio: 3.50

Final: team.scores["criterion-1"] = 3.50 (promedio de 4+3+4+3)
```

---

**Nota:** Si el problema persiste despuÃ©s de revisar los logs y endpoints, puede ser un problema de UI en el frontend donde no se estÃ¡n mostrando los promedios actualizados correctamente.

